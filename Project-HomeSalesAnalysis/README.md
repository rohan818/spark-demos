## Home Sales Analysis using PySpark

### Objectives:
- Utilize SparkSQL to derive essential insights from home sales data.
- Employ Spark functionalities to create temporary views, partition data, cache and uncache temporary tables, and validate the uncaching process.

### Setup Instructions:
1. Install Apache Spark.
2. Create a Python virtual environment and install necessary dependencies:
    - `findspark`
    - `jupyterlab`
3. Run JupyterLab and load the notebook.

### Running the Notebook:
- Ensure that Spark is properly configured and running.
- Open JupyterLab and load the `home_sales_analysis.ipynb` notebook.
- Follow the instructions within the notebook cells to execute code blocks and analyze the data.
