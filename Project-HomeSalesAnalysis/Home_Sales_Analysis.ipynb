{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "075775ea-9d15-4d3c-ba23-87051eede38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b8aa599-3359-4522-b9bb-3f16353c8812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b87d1030-9be4-4148-8337-3b7ce1b82e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "|                  id|      date|date_built| price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "|f8a53099-ba1c-47d...|2022-04-08|      2016|936923|       4|        3|       3167|   11733|     2|         1|  76|\n",
      "|7530a2d8-1ae3-451...|2021-06-13|      2013|379628|       2|        2|       2235|   14384|     1|         0|  23|\n",
      "|43de979c-0bf0-4c9...|2019-04-12|      2014|417866|       2|        2|       2127|   10575|     2|         0|   0|\n",
      "|b672c137-b88c-48b...|2019-10-16|      2016|239895|       2|        2|       1631|   11149|     2|         0|   0|\n",
      "|e0726d4d-d595-407...|2022-01-08|      2017|424418|       3|        2|       2249|   13878|     2|         0|   4|\n",
      "|5aa00529-0533-46b...|2019-01-30|      2017|218712|       2|        3|       1965|   14375|     2|         0|   7|\n",
      "|131492a1-72e2-4a8...|2020-02-08|      2017|419199|       2|        3|       2062|    8876|     2|         0|   6|\n",
      "|8d54a71b-c520-44e...|2019-07-21|      2010|323956|       2|        3|       1506|   11816|     1|         0|  25|\n",
      "|e81aacfe-17fe-46b...|2020-06-16|      2016|181925|       3|        3|       2137|   11709|     2|         0|  22|\n",
      "|2ed8d509-7372-46d...|2021-08-06|      2015|258710|       3|        3|       1918|    9666|     1|         0|  25|\n",
      "|f876d86f-3c9f-42b...|2019-02-27|      2011|167864|       3|        3|       2471|   13924|     2|         0|  15|\n",
      "|0a2bd445-8508-4d8...|2021-12-30|      2014|337527|       2|        3|       1926|   12556|     1|         0|  23|\n",
      "|941bad30-eb49-4a7...|2020-05-09|      2015|229896|       3|        3|       2197|    8641|     1|         0|   3|\n",
      "|dd61eb34-6589-4c0...|2021-07-25|      2016|210247|       3|        2|       1672|   11986|     2|         0|  28|\n",
      "|f1e4cef7-d151-439...|2019-02-01|      2011|398667|       2|        3|       2331|   11356|     1|         0|   7|\n",
      "|ea620c7b-c2f7-4c6...|2021-05-31|      2011|437958|       3|        3|       2356|   11052|     1|         0|  26|\n",
      "|f233cb41-6f33-4b0...|2021-07-18|      2016|437375|       4|        3|       1704|   11721|     2|         0|  34|\n",
      "|c797ca12-52cd-4b1...|2019-06-08|      2015|288650|       2|        3|       2100|   10419|     2|         0|   7|\n",
      "|0cfe57f3-28c2-472...|2019-10-04|      2015|308313|       3|        3|       1960|    9453|     2|         0|   2|\n",
      "|4566cd2a-ac6e-435...|2019-07-15|      2016|177541|       3|        3|       2130|   10517|     2|         0|  25|\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Read from the AWS S3 bucket into a DataFrame.\n",
    "from pyspark import SparkFiles\n",
    "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "homes_df = spark.read.csv(SparkFiles.get(\"home_sales_revised.csv\"), sep=\",\", header=True)\n",
    "homes_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b714ce6f-ad25-4292-87c8-b311b481d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create a temporary view of the DataFrame\n",
    "homes_df.createOrReplaceTempView('home_sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0183605b-20a4-4cf1-bfc4-062e44486aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|YEAR|AVERAGE_PRICE|\n",
      "+----+-------------+\n",
      "|2022|    296363.88|\n",
      "|2021|    301819.44|\n",
      "|2020|    298353.78|\n",
      "|2019|     300263.7|\n",
      "+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Finding the average price for a four bedroom house sold in each year rounded to two decimal places\n",
    "avg_price_4BR = \"\"\"\n",
    "SELECT YEAR(date) as YEAR, ROUND(AVG(price), 2) AS AVERAGE_PRICE FROM home_sales \n",
    "where bedrooms = 4 GROUP BY YEAR ORDER BY YEAR DESC\"\"\"\n",
    "\n",
    "spark.sql(avg_price_4BR).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5acb2d63-03fc-4fbb-93ad-84061d608db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|YEAR|AVERAGE_PRICE|\n",
      "+----+-------------+\n",
      "|2017|    292676.79|\n",
      "|2016|    290555.07|\n",
      "|2015|     288770.3|\n",
      "|2014|    290852.27|\n",
      "|2013|    295962.27|\n",
      "|2012|    293683.19|\n",
      "|2011|    291117.47|\n",
      "|2010|    292859.62|\n",
      "+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Finding the average price of a home for each year the home was built that have 3 bedrooms and 3 bathrooms rounded to two decimal places\n",
    "\n",
    "avg_price_3a = \"\"\"\n",
    "SELECT YEAR(date_built) AS YEAR, ROUND(AVG(price), 2) AS AVERAGE_PRICE\n",
    "FROM home_sales\n",
    "WHERE bedrooms = 3 and bathrooms = 3 GROUP BY YEAR(date_built) ORDER BY YEAR DESC\"\"\"\n",
    "\n",
    "spark.sql(avg_price_3a).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6097af99-9b71-44a5-9233-8e9340bfd300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|YEAR_BUILT|AVERAGE_PRICE|\n",
      "+----------+-------------+\n",
      "|      2017|    280317.58|\n",
      "|      2016|     293965.1|\n",
      "|      2015|    297609.97|\n",
      "|      2014|    298264.72|\n",
      "|      2013|    303676.79|\n",
      "|      2012|    307539.97|\n",
      "|      2011|    276553.81|\n",
      "|      2010|    285010.22|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  5.Finding the average price of a home for each year built that have 3 bedrooms, 3 bathrooms, with two floors, and are greater than or equal to 2,000 square feet rounded to two decimal places\n",
    "\n",
    "avg_price_3b = \"\"\"\n",
    "SELECT YEAR(date_built) AS YEAR_BUILT, ROUND(AVG(price), 2) AS AVERAGE_PRICE FROM home_sales \n",
    "WHERE bedrooms = 3 and bathrooms = 3  and sqft_living >= 2000 and floors = 2 \n",
    "GROUP BY YEAR_BUILT ORDER BY YEAR_BUILT DESC\"\"\"\n",
    "\n",
    "spark.sql(avg_price_3b).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc0ff384-91cf-4ba0-8780-b4e936d301ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|view|AVERAGE_PRICE|\n",
      "+----+-------------+\n",
      "|  99|   1061201.42|\n",
      "|  98|   1053739.33|\n",
      "|  97|   1129040.15|\n",
      "|  96|   1017815.92|\n",
      "|  95|    1054325.6|\n",
      "|  94|    1033536.2|\n",
      "|  93|   1026006.06|\n",
      "|  92|    970402.55|\n",
      "|  91|   1137372.73|\n",
      "|  90|   1062654.16|\n",
      "|  89|   1107839.15|\n",
      "|  88|   1031719.35|\n",
      "|  87|    1072285.2|\n",
      "|  86|   1070444.25|\n",
      "|  85|   1056336.74|\n",
      "|  84|   1117233.13|\n",
      "|  83|   1033965.93|\n",
      "|  82|    1063498.0|\n",
      "|  81|   1053472.79|\n",
      "|  80|    991767.38|\n",
      "+----+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken is: 0.2576727867126465 seconds\n"
     ]
    }
   ],
   "source": [
    "# 6. Finding the \"view\" rating for the average price of a home, rounded to two decimal places, where the homes are greater than or equal to $350,000\n",
    "# and  determinining the run time for this query.\n",
    "\n",
    "start_time = time.time()\n",
    "view_rating=\"\"\"\n",
    "SELECT view, ROUND(AVG(price), 2) AS AVERAGE_PRICE FROM home_sales \n",
    "GROUP BY view HAVING AVG(price) >= 350000 ORDER BY view desc\"\"\"\n",
    "\n",
    "spark.sql(view_rating).show()\n",
    "\n",
    "print(\"Time taken is: %s seconds\" %(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "90b29b66-c77d-4111-803f-a119cdc4210d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Cache the the temporary table home_sales.\n",
    "\n",
    "spark.sql('CACHE TABLE home_sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "20be897b-5457-4050-a60d-49a31b191b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. Checking if the table is cached.\n",
    "\n",
    "spark.catalog.isCached('home_sales')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b0b55ac-de1b-4f97-bd4c-05de21ca6966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|view|AVERAGE_PRICE|\n",
      "+----+-------------+\n",
      "|  99|   1061201.42|\n",
      "|  98|   1053739.33|\n",
      "|  97|   1129040.15|\n",
      "|  96|   1017815.92|\n",
      "|  95|    1054325.6|\n",
      "|  94|    1033536.2|\n",
      "|  93|   1026006.06|\n",
      "|  92|    970402.55|\n",
      "|  91|   1137372.73|\n",
      "|  90|   1062654.16|\n",
      "|  89|   1107839.15|\n",
      "|  88|   1031719.35|\n",
      "|  87|    1072285.2|\n",
      "|  86|   1070444.25|\n",
      "|  85|   1056336.74|\n",
      "|  84|   1117233.13|\n",
      "|  83|   1033965.93|\n",
      "|  82|    1063498.0|\n",
      "|  81|   1053472.79|\n",
      "|  80|    991767.38|\n",
      "+----+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken with data cached is: 0.1631767749786377 seconds\n"
     ]
    }
   ],
   "source": [
    "# 9. Running the query using the cached data,  that filters out the view ratings with average price\n",
    "#  greater than or equal to $350,000 and  the runtime and compare it to uncached runtime.\n",
    "\n",
    "start_time = time.time()\n",
    "view_rating=\"\"\"\n",
    "SELECT view, ROUND(AVG(price), 2) AS AVERAGE_PRICE FROM home_sales \n",
    "GROUP BY view HAVING AVG(price) >= 350000 ORDER BY view desc\"\"\"\n",
    "\n",
    "spark.sql(view_rating).show()\n",
    "\n",
    "print(\"Time taken with data cached is: %s seconds\" %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a8d9d339-49b9-47e3-9fc6-c4d4170d0e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Partitioning by the \"date_built\" field on the formatted parquet home sales data\n",
    "\n",
    "homes_df.write.partitionBy('date_built').parquet('parquet_home_sales', mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "95fef2ab-ace6-4724-935c-b56781c796af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Reading the formatted parquet data\n",
    "\n",
    "parquet_df = spark.read.parquet('parquet_home_sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99f5b2ef-fe74-4be3-a912-1fa6bcb1bf7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('UNCACHE TABLE home_sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "62be395a-b94d-49d4-93d9-38e48eb0d0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken with parquet data is: 3.814697265625e-05 seconds\n"
     ]
    }
   ],
   "source": [
    "# 12. Runing the query that filters out the view ratings with average price of greater than or eqaul to $350,000\n",
    "# with the parquet DataFrame, Rounding the average to two decimal places.\n",
    "# Determining the runtime and compare it to the cached version.\n",
    "\n",
    "start_time = time.time()\n",
    "avg_price_350parquet = \"\"\"\n",
    "SELECT view, ROUND(AVG(price), 2) AS AVERAGE_PRICE FROM parquet_df \n",
    "GROUP BY view HAVING AVG(price) >= 350000 ORDER BY view desc\"\"\"\n",
    "\n",
    "print(\"Time taken with parquet data is: %s seconds\" %(time.time() - start_time))\n",
    "\n",
    "#0.00008249282 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4007f8df-7410-4282-bcc0-6493d61fed6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "home_sales is not cached\n"
     ]
    }
   ],
   "source": [
    "# Checking if the home_sales is no longer cached\n",
    "if spark.catalog.isCached('home_sales'):\n",
    "  print('home_sales is cached')\n",
    "else:\n",
    "  print('home_sales is not cached')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "20d83f11-aa28-4699-bf79-3ea18712d60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"id\": \"driver\",\n",
      "        \"hostPort\": \"172.25.195.172:36539\",\n",
      "        \"isActive\": true,\n",
      "        \"rddBlocks\": 0,\n",
      "        \"memoryUsed\": 73071,\n",
      "        \"diskUsed\": 0,\n",
      "        \"totalCores\": 8,\n",
      "        \"maxTasks\": 8,\n",
      "        \"activeTasks\": 0,\n",
      "        \"failedTasks\": 0,\n",
      "        \"completedTasks\": 17,\n",
      "        \"totalTasks\": 17,\n",
      "        \"totalDuration\": 133335,\n",
      "        \"totalGCTime\": 971,\n",
      "        \"totalInputBytes\": 21361922,\n",
      "        \"totalShuffleRead\": 14987,\n",
      "        \"totalShuffleWrite\": 14987,\n",
      "        \"isBlacklisted\": false,\n",
      "        \"maxMemory\": 455501414,\n",
      "        \"addTime\": \"2024-02-27T03:05:14.472GMT\",\n",
      "        \"executorLogs\": {},\n",
      "        \"memoryMetrics\": {\n",
      "            \"usedOnHeapStorageMemory\": 73071,\n",
      "            \"usedOffHeapStorageMemory\": 0,\n",
      "            \"totalOnHeapStorageMemory\": 455501414,\n",
      "            \"totalOffHeapStorageMemory\": 0\n",
      "        },\n",
      "        \"blacklistedInStages\": [],\n",
      "        \"peakMemoryMetrics\": {\n",
      "            \"JVMHeapMemory\": 184626496,\n",
      "            \"JVMOffHeapMemory\": 250514152,\n",
      "            \"OnHeapExecutionMemory\": 0,\n",
      "            \"OffHeapExecutionMemory\": 0,\n",
      "            \"OnHeapStorageMemory\": 619783,\n",
      "            \"OffHeapStorageMemory\": 0,\n",
      "            \"OnHeapUnifiedMemory\": 619783,\n",
      "            \"OffHeapUnifiedMemory\": 0,\n",
      "            \"DirectPoolMemory\": 451803,\n",
      "            \"MappedPoolMemory\": 0,\n",
      "            \"ProcessTreeJVMVMemory\": 0,\n",
      "            \"ProcessTreeJVMRSSMemory\": 0,\n",
      "            \"ProcessTreePythonVMemory\": 0,\n",
      "            \"ProcessTreePythonRSSMemory\": 0,\n",
      "            \"ProcessTreeOtherVMemory\": 0,\n",
      "            \"ProcessTreeOtherRSSMemory\": 0,\n",
      "            \"MinorGCCount\": 107,\n",
      "            \"MinorGCTime\": 828,\n",
      "            \"MajorGCCount\": 2,\n",
      "            \"MajorGCTime\": 143,\n",
      "            \"TotalGCTime\": 971\n",
      "        },\n",
      "        \"attributes\": {},\n",
      "        \"resources\": {},\n",
      "        \"resourceProfileId\": 0,\n",
      "        \"isExcluded\": false,\n",
      "        \"excludedInStages\": []\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#code snippet that demonstrates how to fetch executor information from the Spark REST API.\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def fetch_executor_details():\n",
    "    # URL for the Spark REST API endpoint for executors\n",
    "    spark_rest_api_url = 'http://localhost:4040/api/v1/applications'\n",
    "    \n",
    "    # Fetch list of applications\n",
    "    apps_response = requests.get(spark_rest_api_url)\n",
    "    apps = apps_response.json()\n",
    "    \n",
    "    if not apps:\n",
    "        print(\"No applications found.\")\n",
    "        return\n",
    "    \n",
    "    # details from the first application\n",
    "    app_id = apps[0]['id']\n",
    "    \n",
    "    # Construct URL to fetch executor details for the application\n",
    "    executors_url = f'{spark_rest_api_url}/{app_id}/executors'\n",
    "    \n",
    "    # Fetch executor details\n",
    "    executors_response = requests.get(executors_url)\n",
    "    executors = executors_response.json()\n",
    "    \n",
    "    # Pretty print the executor details\n",
    "    print(json.dumps(executors, indent=4))\n",
    "\n",
    "# Call the function to fetch and print executor details\n",
    "fetch_executor_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e3a9f22-942c-470b-ab0e-aba05e22ab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
